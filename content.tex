\chapter{Intro}
\label{cha:intro}

When developing an Haskell program and using advanced type system features such as GADTs (Generalized Algebraic Data Types), often the program fails to typecheck without additional information about the structure of the types.
It is thus necessary for the programmer to write proofs to aid the compiler, for example in the form of propositional equality statements.
Usually these proofs take the form of a singleton, a type with a single possible inhabitant that is used to simply carry type information.
We offer an overview of this context in section~\ref{sec:context}.

Such proofs can end up increasing the computational complexity of the program: the body of the proof gets executed together with the caller, and can potentially have a high cost.
An extended example is shown in section~\ref{sec:problem}.
Since the proof can only return one possible value, the unique inhabitant of the singleton, one would expect that, after having typechecked it to verify the soundness of the proof, the compiler optimized it to a constant operation.
This is not the case though, due to the pervasive presence of another inhabitant: $\bot$ (bottom).
In haskell, $\bot$ can inhabit any type, and can come from many sources, for example \texttt{undefined}, \texttt{error}, and most often nontermination.

As explained in chapter~\ref{cha:solution}, we solve this problem by employing a totality checker coupled with a whitelist of inhabited types.
We developed an optimizer that can optimize provably total singletons to a constant operation in a sound way.

We provide the optimizer as a GHC plugin and briefly illustrate its design in section \ref{sec:implementation}.
Its usage (section~\ref{subsec:usage}) is as simple as adding the plugin as a dependency and annotating the relevant bindings.

Finally, in chapter~\ref{cha:results}, we demonstrate how the optimizer can lower the complexity of some proof-heavy expressions.
We show that the complexity of the motivating example is brought down from $O(n^2)$ down to $O(n)$.
Limitations are also discussed.

The optimizer is made available as a Cabal library in a repository located at \url{https://git.sr.ht/~fgaz/singleton-optimizer} and mirrored at \url{https://github.com/fgaz/singleton-optimizer}.

\section{Context}
\label{sec:context}

To be able to properly describe the problem, we first need to recall how GADTs work and why they are used.

GADTs (Generalized Algebraic Data Types) are a powerful generalization of Haskell's standard ADTs (Algebraic Data Types) that allow the programmer to explicitly specify the type signatures of data constructors. To briefly remind the reader of the usefulness of GADTs, an example follows:

\begin{lstlisting}[caption=A GADT describing a simple language with ints and pairs]
data Expr a
  where
    Int    :: Int              -> Expr Int
    Pair   :: Expr a -> Expr b -> Expr (a, b)
    Succ   :: Expr Int         -> Expr Int
    First  :: Expr (a, b)      -> Expr a
\end{lstlisting}

In the above listing, \texttt{Expr} has a type parameter that represents the type of the expression it represents.
This makes it possible to disallow meaningless \texttt{Expr}s.
For example, \texttt{Succ (Int 2)} is well-typed, while \texttt{First (Int 2)} and \texttt{Succ (Pair (Int 3) (Int 1))} are not, because of the type mismatches between \texttt{First} and \texttt{Int} and between \texttt{Succ} and \texttt{Pair}.

Since meaningless terms are not possible, an evaluator for this language is extremely straightforward, without the need for runtime error handling:

\begin{lstlisting}[caption=Evaluator for \texttt{Expr}]
eval :: Expr a -> a
eval (Int i) = i
eval (Pair a b) = (eval a, eval b)
eval (Succ i) = succ (eval i)
eval (First e) = fst (eval e)
\end{lstlisting}

In this example, \texttt{eval} can return an \texttt{Int} when the constructor is \texttt{Int}, because then the type variable \texttt{a} is guaranteed to be \texttt{Int}.

One typical use of GADTs is encoding the length of a list in its type, forming what is commonly called an \emph{indexed vector}, so that the added information allows the programmer to implement otherwise unsafe functions in a safe way:

\begin{lstlisting}[caption=A length-indexed vector]
-- @*Type of the peano naturals.*@
-- @*We lift it to the kind (\texttt{N}) and type (\texttt{'Z} and \texttt{'S}) level by using the \texttt{DataKinds} GHC extension*@
data N = Z | S N

data Vec :: * -> N -> *
  where
    -- @*The empty vector has length zero*@
    Nil  :: Vec a 'Z
    -- @*Each time an element is added, the length is incremented*@
    Cons :: a -> Vec a n -> Vec a ('S n)

head :: Vec a ('S n) -> a
head (Cons x _) = x
-- @*We do not (and cannot) match for the other constructor*@
\end{lstlisting}

In the above example, vectors of different lengths will have different types, with the second type parameter representing the length.
For example, \texttt{Cons 'h' (Cons 'i' Nil)} will have type \texttt{Vec Char ('S ('S 'Z))}, as it contains two \texttt{Char} elements.
We can be sure that the \texttt{head} function will only be called on non-empty lists, since \texttt{Nil} cannot have type \texttt{Vec a ('S n)}.
Attempting to call it on a potentially empty list will result in a compile-time type error.

Let's make another example:

\begin{lstlisting}[caption=Appending a \texttt{Vec} to another]
-- @*Type-level addition*@
type family (n :: N) :+: (m :: N) :: N where
    'Z   :+: m = m
    'S n :+: m = 'S (n :+: m)

append :: Vec a n -> Vec a m -> Vec a (n :+: m)
append Nil         ys = ys
append (Cons x xs) ys = Cons x (append xs ys)
\end{lstlisting}

We first had to define the type-level addition of natural numbers. We do this with a recursive definition like we would do with the normal data-level addition.

The append operation's implementation looks much like its non-indexed (operating on \texttt{[]}s) counterpart, but its type carries a lot more information: it makes explicit the fact that the length of the resulting vector is the sum of the lengths of its arguments.

In all the previous examples, the length-indexed vector managed to hold useful type information without adding significant complexity or inefficiencies.

\section{Problem}
\label{sec:problem}

While GADTs can greatly enhance the expressivity of the type system, in certain cases they can also introduce a potentially avoidable increase in computational complexity.
This is particularly evident in some specific cases.
In the next section we introduce one of them.

\subsection{Motivating example}
\label{subsec:motivating-example}

The problem arises when we try to write more complex functions that the ones in section~\ref{sec:context}.

For example, here is an implementation of the classic $O(n)$ reverse, but on the length-indexed vectors:

\begin{lstlisting}
reverse' :: Vec a n -> Vec a m -> Vec a (n :+: m)
reverse' Nil         acc = acc
reverse' (Cons x xs) acc = reverse' xs (Cons x acc)

reverse :: Vec a n -> Vec a n
reverse v = reverse' v Nil
\end{lstlisting}

Again, the code looks much the same as the non-indexed variant, except that now the type gives us the guarantee that the vector's length will remain exactly the same.
But this time, the above code will not compile, and will generate an error similar to:

\begin{lstlisting}[language=]
Couldn't match type n with n :+: 'Z
  @*[...]*@
Expected type: Vec a n
  Actual type: Vec a (n :+: 'Z)
\end{lstlisting}

in \texttt{reverse} and

\begin{lstlisting}[language=]
Could not deduce: (n1 :+: 'S m) ~ 'S (n1 :+: m)
from the context: n ~ 'S n1
  [...]
Expected type: Vec a (n :+: m)
  Actual type: Vec a (n1 :+: 'S m)
\end{lstlisting}

in \texttt{reverse'}.

The complexity added by GADTs is enough that GHC cannot prove on its own that \texttt{reverse} and \texttt{reverse'} have the types specified in their signatures (which are correct, as we will show).
In the \texttt{reverse} error, GHC is telling us that it is not able to autonomously produce a proof that $n \sim n+0$ (the right addition identity).
In the \texttt{reverse'} error, the same happens for $n+(m+1) \sim (n+m)+1$ (a subset of the associative property).
Tipically, GHC can only prove type equalities by starting from the definitional equality (the equalities derived directly from the type definition), and this is why it is able to prove, for example, $0+n \sim n$ but not $n+0 \sim n$. The former is present in \texttt{:+:}'s definition (\texttt{'Z :+: m = m}), the latter is not.

Those additional proofs need to be supplied by the programmer in the form of a propositional equality declaration, that is, by explicitly writing an expression with return type \texttt{a $:\sim:$ b}, where \texttt{a} and \texttt{b} are the types to be proven equal.

To do this for our example, a few auxiliary definitions are needed:

\begin{lstlisting}[caption=Bringing \texttt{N} to the value level]
-- @*A singleton over natural numbers, needed to work with \texttt{N} at the value level*@
data SN :: N -> * where
    SZ :: SN 'Z
    SS :: SN n -> SN ('S n)

length :: Vec a n -> SN n
length Nil         = SZ
length (Cons _ xs) = SS (length xs)
\end{lstlisting}

We can now apply \texttt{length} to a vector and inspect its size.

In listings \ref{lst:proof-z} and \ref{lst:proof-s} we implement the two proofs needed for \texttt{reverse} to compile:

\begin{lstlisting}[caption=A proof that \texttt{'Z} is the right identity element in type-level addition, label=lst:proof-z]
proofIdentityZ :: SN n -> n :+: 'Z :~: n
proofIdentityZ SZ = Refl
proofIdentityZ (SS n) = case proofIdentityZ n of Refl -> Refl
\end{lstlisting}

\begin{lstlisting}[caption=A proof that $n+(m+1) \sim (n+m)+1$, label=lst:proof-s]
proofS :: SN n -> SN m -> n :+: 'S m :~: 'S (n :+: m)
proofS SZ     _ = Refl
proofS (SS n) m = case proofS n m of Refl -> Refl
\end{lstlisting}

The only inhabitant of \texttt{:$\sim$:}, \texttt{Refl}, represents a known reflexivity.

Both of our proofs work by shrinking the problem until a base case of definitional equality is reached.
The returned \texttt{Refl} is built not by composing other \texttt{Refl}s from the recursive calls, but by using their type information to deduce that the expression is well typed.
To get this information, the result of the recursive call has to be evaluated by the \texttt{case} statement, so both proofs have $O(n)$ complexity.

For example, in \texttt{proofIdentityZ}, the base case directly uses the definitional equality, while the second proves $(n+1) + 0 \sim n+1$ (we match on \texttt{SS n}, hence the $+1$) by recursively calling itself and obtaining $n+0 \sim n$.
The first proposition can be deduced from the second directly by definitional equality:

$$
n+0 \sim n
$$

We add $1$ (apply \texttt{SN}) to both sides:

$$
(n+0)+1 \sim n+1
$$

From the definitional equality derived from \texttt{:+:}, \texttt{'S n :+: m = 'S (n :+: m)}, we have that

$$
(n+0)+1 \sim (n+1)+0
$$

hence

$$
(n+1) + 0 \sim n+1
$$

\fbox{\phantom{\rule{.7ex}{.7ex}}} \\ % no need to depend on amsthm jsut for this

This brings us to the final version of \texttt{reverse}, which provides the compiler with the proofs as evidence that the types shown in the previous error are, indeed, equivalent:

\begin{lstlisting}[label=lst:reversal-final,caption=Length-indexed vector reversal]
reverse' :: Vec a n -> Vec a m -> Vec a (n :+: m)
reverse' Nil         acc = acc
reverse' (Cons x xs) acc =
    case proofS (length xs) (length acc) of
      Refl -> reverse' xs (Cons x acc)

reverse :: Vec a n -> Vec a n
reverse v =
    case proofIdentityZ (length v) of
      Refl -> reverse' v Nil
\end{lstlisting}

\subsection{Complexity issues of the example}
\label{subsec:complexity-issues}

Normally the complexity of a list reversal is $O(n)$.
In this case though we have to take into account the calls to \texttt{proofS}, \texttt{proofIdentityZ}, and \texttt{length}.
While \texttt{proofIdentityZ} is just a single $O(n)$ operation, we can see that \texttt{proofS} and \texttt{length}, each one $O(n)$, get called at every \texttt{reverse'} iteration ($n$ in total), for a final complexity of $O(n^2)$.

In this example, the complexity raises by a factor of $n$, but this overhead (the computation of \texttt{Refl}) is not actually needed to produce the result, it just carries the type information needed to verify the correctness of the algorithm.
Moreover, the \texttt{n :+: 'Z $:\sim:$ n} type has exactly one non-$\bot$ inhabitant, \texttt{Refl}, and this is the case for every valid equality proposition. Such types are commonly called \emph{singletons}, as mentioned in the itroduction.

The compiler though cannot optimize the proof away and substitute it with a type-coerced \texttt{Refl} because, while the representation of an eventual result would be identical, a Haskell program is not guaranteed to terminate. Nontermination can be used to construct unsound proofs, so executing the proof at runtime is necessary to maintain soundness.
This is because, even if an unsoud proof is constucted through nontermination, the actual result of the proof will never be produced.

For example, the following "proof" will compile, but if it terminated it would obviously be unsound:

\begin{lstlisting}[caption=Proving the false through nontermination]
unsound :: Char :~: Bool
unsound = unsound
\end{lstlisting}

The proposition defined by \texttt{unsound}'s type states that \texttt{Char} is equivalent to \texttt{Bool}. This is clearly false, but due to the infinite recursion the program will typecheck. On execution, though, the incorrect proposition will never be returned, and the program will hang indefinitely while trying to prove the false.

If such an expression was optimized to a constant operation, the above would not be valid anymore, and \texttt{unsound} would break the safety of the type system.

In general, every singleton type in Haskell suffers from this issue, which is conversely absent in total languages such as Coq.
Total languages only compile when the program is guaranteed to produce a result in a finite amount of time for every possible input.

\chapter{Solution}
\label{cha:solution}

Since well-typed singletons only have one possible inhabintant for each type variable assignment (we exclude $\bot$ for now), it is possible to construct that term by inspecting the datatype's constructors and picking the only possible one.
The resulting term, with appropriate type coercions, can be substituted to the (already type-checked) proof, and, being the reduced form of the proof, it will have $O(1)$ complexity.

In the case of a function, we can build one with the same arity that ignores its arguments and returns a term constructed in the same way.
Since we have to ensure that the return type is actually inhabited, and by the Curry-Howard isomorphism\footnote{The Curry-Howard isomorphism associates types with logic propositions and terms that inhabit those types to proofs.} the function type corresponds to logic implication, this can be done only at the condition that all the argument types are inhabited (the result of an implication is definitely true if its argument is).

We solve the $\bot$ problem by whitelisting a few useful identifiers from external libraries and by using totality checking.
Totality checking is a form of program analysis that tries to prove termination or nontermination.
Due to the undecidability of the halting problem, there are some cases where determining this is not possible in a finite amount of time.
This means the optimization will not trigger in all possible cases, but the cases where it will are enough to be useful for our purposes.

We only need a partial totality check, localized on the binding to optimize.

\section{Implementation}
\label{sec:implementation}

We implemented the optimization as a GHC Core plugin that we named \texttt{singleton-optimizer}.

The plugin analyzes each top-level binding that is annotated with the \texttt{OptimizeSingleton} annotation and verifies that all the following conditions are met:

\begin{itemize}
  \item{The binding is a singleton, or can return a singleton}
  \item{The binding is total}
  \item{All the referenced bindings are total, recursively}
\end{itemize}

The singleton check is implemented by inspecting the type of the expression.

First, all abstractions (both $\lambda$ and $\Lambda$) are removed, then data constructors are extracted from the resulting type.
For example, in \texttt{forall n . SN n -> n :+: 'Z :$\sim$: n}, we only consider \texttt{n :+: 'Z :$\sim$: n}.

For the type to be detected as a singleton, the type constructor should have only a single data constructor with no arguments.
In the previous example, the type constructor of the result is \texttt{:$\sim$:}, and its only constructor is \texttt{Refl}

Even though it is overly restrictive\footnote{For example, singletons with multiple data constructor of which only one is valid for any given type variable instantiation are discarded.}, this check manages to catch many useful singletons.

\begin{lstlisting}[caption=Some examples of singleton detection]
-- @*This type is correctly identified as a singleton*@
type commutativity n m = n :+: m :~: m :+: n

-- @*This type also passes the check, as the function returns a singleton*@
type commutativity' n m = SN n -> SN m -> n :+: m :~: m :+: n

-- @*These types fail the check as expected*@
data Foo = Foo1 | Foo2
data Baz = Baz Foo

-- @*A more complex type is not detected*@
data SN :: N -> * where
    SZ :: SN 'Z
    SS :: SN n -> SN ('S n)
\end{lstlisting}

Totality checking is implemented by leveraging the Liquid Haskell\cite{refinement-types-for-haskell} totality checker\footnote{Only in structural mode at the time of writing. Enabling the metric-based checker is planned.}.
It was chosen mainly because, being an active project written in Haskell itself and made available as both an executable and a library that acts on GHC Core, it makes integrating it with the GHC plugin system fairly straightforward.

Liquid Haskell only checks definitions and not uses\footnote{And it would not be possible to check all use sites anyway, since dependant modules are not known beforehand.}, and Haskell does not distinguish between data (in which infinite structures are not possible) and codata (in which they are), so in principle all functions can fail to terminate even if they are structurally recursive. For example \texttt{length :: [a] -> Int} is structurally recursive, but can hang if it is passed an infinite list.
This limits us to optimizing only non-function types though, which is extremely limiting, but, as explained in the previous section, we can lift this restriction if all the arguments are inhabited (by a non-$\bot$ term).
This may come at the cost of increased laziness (see section~\ref{subsec:limitations-laziness}).

Programmatically checking a type for inhabitants is a difficult task in itself, so in our work we simply provide a list of manually checked types.
Only types from this whitelist are allowed as arguments of an optimizable function.
Most importantly, the \texttt{SN} type is part of the allowed ones.

\subsection{Usage}
\label{subsec:usage}

The plugin is published as a Cabal library in a repository located at \url{https://git.sr.ht/~fgaz/singleton-optimizer} and mirrored at \url{https://github.com/fgaz/singleton-optimizer}
To use it, either install it with the command \texttt{cabal install singleton-optimizer} and use it by manually specifying the \texttt{-fplugin GHC.Plugin.SingletonOptimizer} GHC option, or add it as a dependency and GHC option in the .cabal file of the package you need to optimize.
Listing~\ref{lst:cabal} provides an example.

\begin{lstlisting}[label=lst:cabal, caption=Example .cabal stanza]
library somelib
  ghc-options:         -fplugin GHC.Plugin.SingletonOptimizer
  build-depends:       base, singleton-optimizer
  exposed-modules:     YourModule
  hs-source-dirs:      src
  default-language:    Haskell2010
\end{lstlisting}

Once the package is installed or specified as a dependency, add the \texttt{OptimizeSingleton} annotation to the singleton declarations that need to be optimized.

\begin{lstlisting}[caption=Optimizing \texttt{proofS}]
{-# ANN proofS OptimizeSingleton #-}
proofS :: SN n -> SN m -> n :+: 'S m :~: 'S (n :+: m)
proofS SZ     _ = Refl
proofS (SS n) m = case proofS n m of Refl -> Refl
\end{lstlisting}

This is the same code as listing~\ref{lst:proof-s}, but we added the \texttt{OptimizeSingleton} annotation just before the binding to let the plugin optimize it.

In case the totality checker cannot prove that one of the bindings terminates (a warning will be given), but the termination is manually proven, one can use the \texttt{UnsafeTotal} annotation to force the plugin to treat that binding as total.
Obviously this annotation is not to be used lightly, as it can break the soundness of the optimization if used on a non-total binding.

\chapter{Results}
\label{cha:results}

We now show that the optimization produces a real and measurable decrease in complexity, by means of tests and bennchmarks.
We also describe the limitations to be aware of when employing the plugin.

\section{Tests and Core output}
\label{sec:tests}

A number of tests with increasing complexity was written and categorized depending on the expected and actual triggering of the optimization.
There are some cases where the plugin could optimize a singleton and it does not, but this does not undermine the soundness of the programs.
More importantly, there is only one case where the plugin unsoundly optimizes where it shouldn't, described in section \ref{subsec:limitations-lh}, and it's due to a bug in Liquid Haskell.

The core output of the plugin when ran on the tests was successfully sanity-checked with the \texttt{-dcore-lint} GHC option.
Furthermore, its external representation\cite{core-external-representation} was manually verified for adherence to the expected output.

For example, this is one of the simplest tests:

\begin{lstlisting}
TODO, I don't have the toolchain needed to print the core on this machine
\end{lstlisting}

Normally, this is the Core it would produce:

\begin{lstlisting}
TODO, I don't have the toolchain needed to print the core on this machine
\end{lstlisting}

When we enable the optimization, instead, the body of the function is replaced by a correctly coerced \texttt{Refl}:

\begin{lstlisting}
TODO, I don't have the toolchain needed to print the core on this machine
\end{lstlisting}

\section{Benchmarks}
\label{sec:benchmarks}

We now proceed to analyze the effects of the application of the optimization on our motivating example.

We ran a benchmark in which \texttt{Vec}s of progressively larger sizes were reversed using the \texttt{reverse} function defined in listing~\ref{lst:reversal-final}.
We then ran the same benchmark after adding the \texttt{OptimizeSingleton} annotation to the \texttt{proofIdentityZ} and \texttt{proofS} proofs, bringing their computational complexities down to a constant.
Gone the overhead of the proofs, the complexity of the optimized \texttt{reverse} should be $O(n)$.

This is confirmed by the results of the benchmark, shown in figure~\ref{fig:bench-reverse}.

\begin{figure}
  \begin{minipage}{0.5\textwidth}
    \begin{tikzpicture}
    \begin{axis}
      [ title = Unoptimized list reverse
      , xlabel = List length
      , ylabel = Time (s)
      , error bars/y dir = both
      , error bars/y explicit
      , error bars/error bar style = { line width=0.5pt }
      , error bars/error mark options =
          { rotate=90
          , red
          , mark size=4pt
          , line width=0.5pt
          }
      ]
      \addplot table
        [ x = N
        , y = Mean
        , y error = Stddev
        , col sep = comma
        , only marks
        ]
        {benchmark-unoptimized.csv};
      \addplot[mark=none, domain=10000:40000]{1.449e-8*x^2 - 1.884e-4*x + 1.411};
    \end{axis}
    \end{tikzpicture}
    \label{fig:bench-reverse:unoptimized}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
    \begin{tikzpicture}
    \begin{axis}
      [ title = Optimized list reverse
      , xlabel = List length
      , ylabel = Time(s)
      , error bars/y dir = both
      , error bars/y explicit
      , error bars/error bar style = { line width=0.5pt }
      , error bars/error mark options =
          { rotate=90
          , red
          , mark size=4pt
          , line width=0.5pt
          }
      ]
      \addplot table
        [ x = N
        , y = Mean
        , y error = Stddev
        , col sep = comma
        , only marks
        ]
        {benchmark-optimized.csv};
      \addplot[mark=none, domain=10000:40000]{2.209e-7*x - 0.002};
    \end{axis}
    \end{tikzpicture}
    \label{fig:bench-reverse:optimized}
  \end{minipage}
  \caption{Measured times for a length-indexed vector reverse on vectors of varying sizes, with appropriate quadratic and linear fits. On the left, the unoptimized $O(n^2)$ version. On the right, the optimized $O(n)$ one. $R^2$ values are respectively $0.999$ and $0.992$. Note the difference in scale.}
  \label{fig:bench-reverse}
\end{figure}

In the measured range the optimized version is three orders of magnitude faster that the unoptimized one, but most importantly it we can confirm the predicted complexities by fitting the data with the respective polynomials (quadratic and linear) and obtaining a low error.

This benchmark can be repeated by running the \texttt{reverse} benchmark included in the plugin's package with the command \texttt{cabal bench}.

\section{Known limitations}
\label{sec:limitations}

While we believe the implementation offers enough functionality to be useful in a practical setting, there are a few limitations to be considered.

\subsection{The optimization is restricted to individual modules}
\label{subsec:single-module}

Due to limitations in the GHC plugin system the optimizer cannot perform intra-module termination analysis, so the optimization will trigger only if all the names referenced by the annotated binding are either whitelisted or defined in the same module.

\subsection{Increased laziness}
\label{subsec:limitations-laziness}

A side effect of replacing a function body with a constant is that the arguments do not get evaluated at all.
As explained in Section~\ref{cha:solution}, this does not cause problems with the soundness of the proof, but it does cause an increase in the function's laziness.
For example, passing \texttt{undefined} as an argument will not make the optimized program crash when the unoptimized one would have.
We do not consider this as a significant problem, since the programmer is only interested in the proof, which is guaranteed to hold anyway.

\subsection{Limitations derived from Liquid Haskell}
\label{subsec:limitations-lh}

In some cases, Liquid Haskell's totality checker can report a function as total when it actually is not.

Such known cases are, at the time of writing:

\begin{description}
  \item[Datatype-encoded recursion]
    For example, the following code is detected as total and the \texttt{callsRecWithData} obviously incorrect equality is optimized:
    \begin{lstlisting}
newtype Rec a = MkRec (Rec a -> a)

recWithData :: Rec a -> a
recWithData r@(MkRec f) = f r

{-# ANN callsRecWithData OptimizeSingleton #-}
callsRecWithData :: (Int :~: Bool)
callsRecWithData = recWithData (MkRec recWithData)
    \end{lstlisting}

   This error is a known bug in Liquid Haskell\footnote{\url{https://github.com/ucsd-progsys/liquidhaskell/issues/159}}.
\end{description}

\chapter{Conclusions}
\label{cha:conclusions}

We showed how singleton proofs can be reduced to a constant when they are proved total and explained why this optimization isn't already done by GHC.
We developed the \texttt{singleton-optimizer} package, which can detect those cases though the use of state-of-the-art tools, and can optimize them while maintaining soundness.
Finally, we showed that the plugin has a practical and measurable effect.

We believe that the advanced type system features combined with similarly advanced analysis techniques can not only greatly enhance the expressivity of a language, but often can do so at no additional computational cost or can even provide the means for further optimizations.

Furthermore, as shown in this work, they can be applied to a restricted part of the program to great effect, without overly restricting the rest of it (in this optimization the totality check was local only).

%MAYBE We believe more languages will shift towards this paradigm of

