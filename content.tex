\chapter{Intro}
\label{cha:intro}

When developing an Haskell program and using advanced type system features such as GADTs (Generalized Algebraic Data Types), sometimes the program fails to typecheck without additional information about the structure of the types.
It is thus necessary for the programmer to write proofs to aid the compiler, for example in the form of propositional equality statements.
Usually these proofs have a singleton type, a type with a single inhabitant that is used to simply carry type information.
We offer an overview of this context in section~\ref{sec:context}.

Such proofs can end up increasing the computational complexity of the program: in Haskell the body of the proof must be executed together with the caller, and can potentially have a high cost.
An extended example is shown in section~\ref{sec:problem}.
Since the proof can only return one value, the unique inhabitant of the singleton, one would expect that, after having typechecked it to verify the soundness of the proof, the compiler optimized it to a constant operation.
This is not the case though, due to the presence of another inhabitant: $\bot$ (bottom).
In Haskell, $\bot$ can inhabit any type, and can come from many sources, for example \texttt{undefined}, \texttt{error}, and most often nontermination.

As explained in chapter~\ref{cha:solution}, we solve this problem by employing a totality checker coupled with a whitelist of inhabited types.
We developed an optimizer that can optimize provably total singleton proofs to a constant operation in a sound way.

We provide the optimizer as a GHC plugin and briefly illustrate its design in section \ref{sec:implementation}.
Its usage (section~\ref{subsec:usage}) is as simple as adding the plugin as a dependency and annotating the relevant bindings.

Finally, in chapter~\ref{cha:results}, we demonstrate how the optimizer can lower the complexity of some proof-heavy expressions.
We show that the complexity of the motivating example is brought down from $O(n^2)$ down to $O(n)$.
Limitations are also discussed.

The optimizer is made available as a Cabal package in a repository located at \url{https://git.sr.ht/~fgaz/singleton-optimizer} and mirrored at \url{https://github.com/fgaz/singleton-optimizer}.

\section{Context}
\label{sec:context}

To be able to properly describe the problem, we first need to recall how GADTs work and why they are used.

GADTs (Generalized Algebraic Data Types) are a powerful generalization of Haskell's standard ADTs (Algebraic Data Types) that allow the programmer to explicitly specify the type signatures of data constructors\cite{simple-unification-based-type-inference-for-gadts}.
An explicit signature can be strictly more expressive than the standard syntax, since it can, for example, restrict the type variables of the return type to a specific instantiation depending on the constructor.
Similarly, given a type variable instantiation, we can know the type constructor that was used, or at least restrict the subset of possible ones.

In general, GADTs can be used to encode properties of the data in its type and enforce invariants in functions.

To briefly remind the reader of the usefulness of GADTs, an example follows:

\begin{lstlisting}[caption=A GADT describing a simple language with ints and pairs]
data Expr a
  where
    Int    :: Int              -> Expr Int
    Pair   :: Expr a -> Expr b -> Expr (a, b)
    Succ   :: Expr Int         -> Expr Int
    First  :: Expr (a, b)      -> Expr a
\end{lstlisting}

In the above listing, \texttt{Expr} has a type parameter that represents the type of the expression it represents.
This makes it possible to disallow meaningless \texttt{Expr}s.
For example, \texttt{Succ (Int 2)} is well-typed, while \texttt{First (Int 2)} and \texttt{Succ (Pair (Int 3) (Int 1))} are not, because of the type mismatches between \texttt{First} and \texttt{Int} and between \texttt{Succ} and \texttt{Pair}.

Since meaningless terms are not represented, an evaluator for this language is extremely straightforward to write, without the need for runtime error handling:

\begin{lstlisting}[caption=Evaluator for \texttt{Expr}]
eval :: Expr a -> a
eval (Int i) = i
eval (Pair a b) = (eval a, eval b)
eval (Succ i) = succ (eval i)
eval (First e) = fst (eval e)
\end{lstlisting}

In this example, \texttt{eval} can return an \texttt{Int} when the constructor is \texttt{Int}, because then the type variable \texttt{a} is guaranteed to be \texttt{Int}.

One typical use of GADTs is encoding the length of a list in its type, forming what is commonly called an \emph{indexed vector}, so that the added information allows the programmer to implement otherwise unsafe functions in a safe way:

\begin{lstlisting}[caption=A length-indexed vector]
-- @*Type of the Peano naturals.*@
-- @*We lift it to the kind (\texttt{N}) and type (\texttt{'Z} and \texttt{'S}) level by using the \texttt{DataKinds} GHC extension*@
data N = Z | S N

data Vec :: * -> N -> *
  where
    -- @*The empty vector has length zero*@
    Nil  :: Vec a 'Z
    -- @*Each time an element is added, the length is incremented*@
    Cons :: a -> Vec a n -> Vec a ('S n)

head :: Vec a ('S n) -> a
head (Cons x _) = x
-- @*We do not (and cannot) match against the other constructor*@
\end{lstlisting}

In the above example, vectors of different lengths will have different types, with the second type parameter representing the length.
For example, \texttt{Cons 'h' (Cons 'i' Nil)} will have type \texttt{Vec Char ('S ('S 'Z))}, as it contains two \texttt{Char} elements.
We can be sure that the \texttt{head} function will only be called on non-empty lists, since \texttt{Nil} cannot have type \texttt{Vec a ('S n)}.
Attempting to call it on a potentially empty list will result in a compile-time type error.

Let's make another example:

\begin{lstlisting}[caption=Appending a \texttt{Vec} to another]
-- @*Type-level addition*@
type family (n :: N) :+: (m :: N) :: N where
    'Z   :+: m = m
    'S n :+: m = 'S (n :+: m)

append :: Vec a n -> Vec a m -> Vec a (n :+: m)
append Nil         ys = ys
append (Cons x xs) ys = Cons x (append xs ys)
\end{lstlisting}

We first had to define the type-level addition of natural numbers. We do this with a recursive definition like we would do with the normal data-level addition.

The append operation's implementation looks much like its non-indexed (operating on \texttt{[]}s) counterpart, but its type carries a lot more information: it makes explicit the fact that the length of the resulting vector is the sum of the lengths of its arguments.

In all the previous examples, the length-indexed vector managed to hold useful type information without adding significant complexity or inefficiencies.

\section{Problem}
\label{sec:problem}

While GADTs can greatly enhance the expressivity of the type system, in certain cases they can also introduce a potentially avoidable increase in computational complexity.
This is particularly evident in some specific cases.
In the next section we introduce one of them.

\subsection{Motivating example}
\label{subsec:motivating-example}

The problem arises when we try to write more complex functions that the ones in section~\ref{sec:context}.

For example, we present an implementation of the classic $O(n)$ reverse, but on the length-indexed vectors.
We first define an helper function that takes two \texttt{Vec}s, and uses them as stacks by recursively popping elements from the first one while pushing them on the second, in reverse order:

\begin{lstlisting}
reverse' :: Vec a n -> Vec a m -> Vec a (n :+: m)
reverse' Nil         acc = acc
reverse' (Cons x xs) acc = reverse' xs (Cons x acc)
\end{lstlisting}

From the type, the length of the returned \texttt{Vec} will be equal to the sum of the lengths of the inputs (as we merge the two).

Then we write a wrapper, which simply passes the \texttt{Vec} to reverse to \texttt{reverse'} in addition to an empty \texttt{Vec}, where the elements will be pushed.

\begin{lstlisting}
reverse :: Vec a n -> Vec a n
reverse v = reverse' v Nil
\end{lstlisting}

Since the type of \texttt{Nil} is \texttt{Vec a 'Z}, the types of the two functions encode the invariant that the sum of the elements of the two \texttt{Vec}s is always equal to the number of elements of the original \texttt{Vec}.

Again, the code looks much the same as the non-indexed variant, except that now the type gives us the guarantee that the vector's length will remain exactly the same.
But this time, the above code will not compile, and will generate an error similar to:

\begin{lstlisting}[language=]
Couldn't match type n with n :+: 'Z
  @*[...]*@
Expected type: Vec a n
  Actual type: Vec a (n :+: 'Z)
\end{lstlisting}

in \texttt{reverse} and

\begin{lstlisting}[language=]
Could not deduce: (n1 :+: 'S m) ~ 'S (n1 :+: m)
from the context: n ~ 'S n1
  [...]
Expected type: Vec a (n :+: m)
  Actual type: Vec a (n1 :+: 'S m)
\end{lstlisting}

in \texttt{reverse'}.

The complexity added by GADTs is enough that GHC cannot prove on its own that \texttt{reverse} and \texttt{reverse'} could safely be given the types specified in the signatures.
In the \texttt{reverse} error, GHC is telling us that it is not able to autonomously produce a proof that $n \sim n+0$ (the right addition identity).
In the \texttt{reverse'} error, the same happens for $n+(m+1) \sim (n+m)+1$ (a special case of the associative property).
Typically, GHC can only prove type equalities by starting from the definitional equality (the equalities derived directly from the type definition), and this is why it is able to prove, for example, $0+n \sim n$ but not $n+0 \sim n$. The former is present in \texttt{:+:}'s definition (\texttt{'Z :+: m = m}), the latter is not.

Those additional proofs need to be supplied by the programmer in the form of a propositional equality declaration, that is, by explicitly writing an expression with return type \texttt{a $:\sim:$ b}, where \texttt{a} and \texttt{b} are the types to be proven equal.

To do this for our example, a few auxiliary definitions are needed:

\begin{lstlisting}[caption=Bringing \texttt{N} to the value level]
-- @*A singleton over natural numbers, needed to work with \texttt{N} at the value level*@
data SN :: N -> * where
    SZ :: SN 'Z
    SS :: SN n -> SN ('S n)

length :: Vec a n -> SN n
length Nil         = SZ
length (Cons _ xs) = SS (length xs)
\end{lstlisting}

We can now apply \texttt{length} to a vector and inspect its size.

In listings \ref{lst:proof-z} and \ref{lst:proof-s} we implement the two proofs needed for \texttt{reverse} to compile:

\begin{lstlisting}[caption=A proof that \texttt{'Z} is the right identity element in type-level addition, label=lst:proof-z]
proofIdentityZ :: SN n -> n :+: 'Z :~: n
proofIdentityZ SZ = Refl
proofIdentityZ (SS n) = case proofIdentityZ n of Refl -> Refl
\end{lstlisting}

\begin{lstlisting}[caption=A proof that $n+(m+1) \sim (n+m)+1$, label=lst:proof-s]
proofS :: SN n -> SN m -> n :+: 'S m :~: 'S (n :+: m)
proofS SZ     _ = Refl
proofS (SS n) m = case proofS n m of Refl -> Refl
\end{lstlisting}

The only inhabitant of \texttt{:$\sim$:}, \texttt{Refl}, represents reflexivity.

Both of our proofs work by shrinking the problem until a base case of definitional equality is reached.
The returned \texttt{Refl} is built not by composing other \texttt{Refl}s from the recursive calls, but by using their type information to deduce that the expression is well typed.
To get this information, the result of the recursive call has to be evaluated by the \texttt{case} statement, so both proofs have $O(n)$ complexity.

For example, in \texttt{proofIdentityZ}, the base case directly uses the definitional equality, while the recursive case proves $(n+1) + 0 \sim n+1$ (we match on \texttt{SS n}, hence the $+1$) by recursively calling itself and obtaining $n+0 \sim n$.
The first proposition can be deduced from the second directly by definitional equality.

Starting from

$$
n+0 \sim n
$$

We add $1$ (apply \texttt{SN}) to both sides:

$$
(n+0)+1 \sim n+1
$$

From the definitional equality derived from \texttt{:+:}, \texttt{'S n :+: m = 'S (n :+: m)}, we have that

$$
(n+1)+0 \sim (n+0)+1
$$

hence

$$
(n+1) + 0 \sim n+1
$$

\fbox{\phantom{\rule{.7ex}{.7ex}}} \\ % no need to depend on amsthm jsut for this

GHC is able to automatically perform the above steps once it is provided with the base case and recursive call defined in \texttt{proofIdentityZ}.

A similar inductive reasoning is done for \texttt{proofS}.

This brings us to the final version of \texttt{reverse}, which provides the compiler with the proofs as evidence that the types shown in the previous error are, indeed, equivalent:

\begin{lstlisting}[label=lst:reversal-final,caption=Length-indexed vector reversal]
reverse' :: Vec a n -> Vec a m -> Vec a (n :+: m)
reverse' Nil         acc = acc
reverse' (Cons x xs) acc =
    case proofS (length xs) (length acc) of
      Refl -> reverse' xs (Cons x acc)

reverse :: Vec a n -> Vec a n
reverse v =
    case proofIdentityZ (length v) of
      Refl -> reverse' v Nil
\end{lstlisting}

\subsection{Complexity issues of the example}
\label{subsec:complexity-issues}

Normally the complexity of a list reversal is $O(n)$.
In this case though we have to take into account the calls to \texttt{proofS}, \texttt{proofIdentityZ}, and \texttt{length}.
While \texttt{proofIdentityZ} is just a single $O(n)$ operation, we can see that \texttt{proofS} and \texttt{length}, each one $O(n)$, get called at every \texttt{reverse'} iteration ($n$ in total), for a total complexity of $O(n^2)$.

In this example, the complexity raises by a factor of $n$, but this overhead (the computation of \texttt{Refl}) is not actually needed to produce the result, it just carries the type information needed to verify the correctness of the algorithm.
Moreover, the \texttt{n :+: 'Z $:\sim:$ n} type has exactly one non-$\bot$ inhabitant, \texttt{Refl}, and this is the case for every valid equality proposition. Such types are commonly called \emph{singletons}, as mentioned in the introduction.

The compiler though cannot optimize the proof away and substitute it with a type-coerced \texttt{Refl} because, while the representation of an eventual result would be identical, a Haskell program is not guaranteed to terminate. Nontermination can be used to construct unsound proofs, so executing the proof at runtime is necessary to maintain soundness and type safety.
This is because, even if an unsound proof is constructed through nontermination, the actual result of the proof will never be produced.

For example, the following "proof" will compile, but if it terminated it would obviously be unsound:

\begin{lstlisting}[caption=Proving the false through nontermination]
unsound :: Char :~: Bool
unsound = unsound
\end{lstlisting}

The proposition defined by \texttt{unsound}'s type states that \texttt{Char} is equivalent to \texttt{Bool}. This is clearly false, but due to the infinite recursion the program will typecheck. On execution, though, the incorrect proposition will never be returned, and the program will hang indefinitely while trying to prove the false.

If such an expression was optimized to a constant operation, the above would not be valid anymore, and \texttt{unsound} would break the safety of the type system, for example by allowing this function to return:

\begin{lstlisting}
unsafeCharToBool :: Char -> Bool
unsafeCharToBool = castWith unsound
\end{lstlisting}

In general, every singleton proof in Haskell suffers from this issue, which is conversely absent in total languages such as Coq.
Total languages only compile when the program is guaranteed to produce a result in a finite amount of time for every possible input.

\chapter{Solution}
\label{cha:solution}

Since singleton indexed types only have one inhabitant for each type variable assignment (we exclude $\bot$ for now), intuitively it is possible to directly construct the normal form of that inhabitant, and to substitute it (with appropriate type coercions) to the already type-checked proof, reducing its complexity to the bare minimum.

In the particular but extremely common case where the singleton type only has one data constructor and each of its arguments also respects this property\footnote{\texttt{:$\sim$:} is such a type, having only the \texttt{Refl} data constructor.}, the inhabitant will always be the same, namely the one built with that only data constructor (recursively for its arguments), regardless of the type argument instantiation.
Therefore, the construction of this type of term can be done entirely at compile time, yielding a $O(1)$ runtime complexity.

In practice, a typechecked binding of the form:

\begin{lstlisting}
x :: S
x = someExpensiveOperation
\end{lstlisting}

where \texttt{S} is a singleton with an inhabitant named \texttt{SInhabinant}, could in principle be optimized to:

\begin{lstlisting}
x :: S
x = unsafeCoerce SInhabitant
\end{lstlisting}

In the case of a function, we can build one with the same arity that ignores its arguments and returns a term constructed in the same way.
Since we have to ensure that the return type is actually inhabited, and by the Curry-Howard isomorphism\footnote{The Curry-Howard isomorphism associates types with logic propositions and terms that inhabit those types to proofs.} the function type corresponds to logic implication, this can be done only at the condition that all the argument types are inhabited (the result of an implication is definitely true if its argument is).

For example, the \texttt{proofIdentityZ} proof defined in listing~\ref{lst:proof-z} could be optimized to:

\begin{lstlisting}
proofIdentityZ :: SN n -> n :+: 'Z :~: n
proofIdentityZ _ = unsafeCoerce Refl
\end{lstlisting}

reducing its complexity from $O(n)$ to a constant.
\texttt{SN} is inhabited, so the optimization is possible.

As a counterexample, the following function cannot be optimized without breaking type safety, because \texttt{'Z :$\sim$: 'S 'Z} is not inhabited:

\begin{lstlisting}
f :: 'Z :~: 'S 'Z -> 'Z :~: 'S 'Z
f = id
\end{lstlisting}

If we did optimize it, then it would be possible to obtain a \texttt{'Z :$\sim$: 'S 'Z} by simply passing \texttt{undefined} to \texttt{f}. The optimized function would ignore it and return a \texttt{'Z :$\sim$: 'S 'Z} without erroring.

We solve the problem that would arise with the presence of $\bot$ in the proof's body by whitelisting a few useful identifiers from external libraries and by using totality checking.
Totality checking is a form of program analysis that tries to prove termination or nontermination.
Due to the undecidability of the halting problem, there are some cases where determining this is not possible in a finite amount of time.
This means that our optimization will not trigger in all possible cases, but the cases where it will are enough to be useful for our purposes.

We only need a partial totality check, localized on the binding to optimize.

\section{Implementation}
\label{sec:implementation}

We implemented the optimization as a GHC Core plugin\cite{ghc-docs-extending}\cite{ghc-wiki}\cite{ghc-library-docs} that we named \texttt{singleton-optimizer}.

The plugin analyzes each top-level binding that is annotated with the \texttt{OptimizeSingleton} annotation and verifies that all the following conditions are met:

\begin{itemize}
  \item{The binding is a singleton, or can return a singleton}
  \item{The binding is total}
  \item{All the referenced bindings are total, recursively}
\end{itemize}

The singleton check is implemented by inspecting the type of the expression.

First, all abstractions (both $\lambda$ and $\Lambda$) are removed, then data constructors are extracted from the resulting type.
For example, in \texttt{forall n . SN n -> n :+: 'Z :$\sim$: n}, we only consider \texttt{n :+: 'Z :$\sim$: n}.

For the type to be detected as a singleton, the type constructor should have only a single data constructor with no arguments.
In the previous example, the type constructor of the result is \texttt{:$\sim$:}, and its only constructor is \texttt{Refl}

Even though it is overly restrictive\footnote{For example, singletons with multiple data constructor of which only one is valid for any given type variable instantiation are discarded.}, this check manages to catch many useful cases.

\begin{lstlisting}[caption=Some examples of singleton detection]
-- @*This type is correctly identified as a singleton*@
type commutativity n m = n :+: m :~: m :+: n

-- @*This type also passes the check, as the function returns a singleton*@
type commutativity' n m = SN n -> SN m -> n :+: m :~: m :+: n

-- @*These types fail the check as expected*@
data Foo = Foo1 | Foo2
data Baz = Baz Foo

-- @*A more complex type is not detected*@
data SN :: N -> * where
    SZ :: SN 'Z
    SS :: SN n -> SN ('S n)
\end{lstlisting}

Totality checking is implemented by leveraging the LiquidHaskell\cite{refinement-types-for-haskell}\cite{liquidhaskell-website} totality checker\footnote{Only in structural mode at the time of writing. Enabling the metric-based checker is planned.}.
It was chosen mainly because, being an active project written in Haskell itself and made available as both an executable and a library that acts on GHC Core, it makes integrating it with the GHC plugin system fairly straightforward.

LiquidHaskell only checks definitions and not uses\footnote{And it would not be possible to check all use sites anyway, since dependant modules are not known beforehand.}, and Haskell allows codata (in which infinite structures are possible), so in principle all functions can fail to terminate even if they are structurally recursive. For example \texttt{length :: [a] -> Int} is structurally recursive, but can hang if it is passed an infinite list.
This would limit us to optimizing only non-function types though, which is extremely limiting, but, as explained in the previous section, we can lift this restriction if all the arguments are inhabited (by a non-$\bot$ term).
This may come at the cost of increased laziness (see section~\ref{subsec:limitations-laziness}).

Programmatically checking a type for inhabitants is a difficult task in itself, so in our work we simply provide a list of manually checked types.
Only types from this whitelist are allowed as arguments of an optimizable function.
Most importantly, the \texttt{SN} type is part of the allowed ones.

\subsection{Usage}
\label{subsec:usage}

The plugin is published as a Cabal library in a repository located at \url{https://git.sr.ht/~fgaz/singleton-optimizer} and mirrored at \url{https://github.com/fgaz/singleton-optimizer}
To use it, either install it with the command \texttt{cabal install singleton-optimizer} and use it by manually specifying the \texttt{-fplugin GHC.Plugin.SingletonOptimizer} GHC option, or add it as a dependency and GHC option in the .cabal file of the package you need to optimize.
Listing~\ref{lst:cabal} provides an example.

\begin{lstlisting}[label=lst:cabal, caption=Example .cabal stanza]
library somelib
  ghc-options:         -fplugin GHC.Plugin.SingletonOptimizer
  build-depends:       base, singleton-optimizer
  exposed-modules:     YourModule
  hs-source-dirs:      src
  default-language:    Haskell2010
\end{lstlisting}

Once the package is installed or specified as a dependency, add the \texttt{OptimizeSingleton} annotation to the singleton proof declarations that need to be optimized.

\begin{lstlisting}[caption=Optimizing \texttt{proofS}]
{-# ANN proofS OptimizeSingleton #-}
proofS :: SN n -> SN m -> n :+: 'S m :~: 'S (n :+: m)
proofS SZ     _ = Refl
proofS (SS n) m = case proofS n m of Refl -> Refl
\end{lstlisting}

This is the same code as listing~\ref{lst:proof-s}, but we added the \texttt{OptimizeSingleton} annotation just before the binding to let the plugin optimize it.

In case the totality checker cannot prove that one of the bindings terminates (a warning will be given), but the termination is manually proven, one can use the \texttt{UnsafeTotal} annotation to force the plugin to treat that binding as total.
Obviously this annotation is not to be used lightly, as it can break the soundness of the optimization if used on a non-total binding.

\chapter{Results}
\label{cha:results}

We now show that the optimization produces a real and measurable decrease in complexity, by means of tests and benchmarks.
We also describe the limitations to be aware of when employing the plugin.

\section{Tests and Core output}
\label{sec:tests}

A number of tests with increasing complexity was written and categorized depending on the expected and actual triggering of the optimization.
There are some cases where the plugin could optimize a singleton and it does not, but this does not undermine the soundness of the programs.
More importantly, there is only one known case where the plugin unsoundly optimizes where it shouldn't, described in section \ref{subsec:limitations-lh}, and it's due to a bug in LiquidHaskell.

The Core\footnote{Core is an intermediate representation of Haskell code based on System FC. See \url{https://gitlab.haskell.org/ghc/ghc/wikis/commentary/compiler/core-syn-type} for more information.} output of the plugin when ran on the tests was successfully sanity-checked with the \texttt{-dcore-lint} GHC option.
Furthermore, its external representation\cite{core-external-representation} was manually verified for adherence to the expected output.

For example, this is one of the simplest tests, a structurally terminating function that returns \texttt{()}:

\begin{lstlisting}
{-# NOINLINE structurallyTerminating #-}
structurallyTerminating :: N -> ()
structurallyTerminating Z = ()
structurallyTerminating (S n) = structurallyTerminating n
\end{lstlisting}

Note that we added a \texttt{NOINLINE} pragma to keep the binding at the top level.
Otherwise, the aggressive GHC inliner would have inlined it and we wouldn't be able to see the result of the optimization as easily.
This is not necessary outside of test code.

Normally, this is the Core that would be produced:

\begin{lstlisting}[language=]
Rec {
-- RHS size: {terms: 8, types: 3, coercions: 0, joins: 0/0}
structurallyTerminating [InlPrag=NOINLINE, Occ=LoopBreaker]
  :: N -> ()
[GblId, Arity=1, Caf=NoCafRefs, Unf=OtherCon []]
structurallyTerminating
  = \ (ds_dpXI :: N) ->
      case ds_dpXI of {
        Z -> GHC.Tuple.();
        S n_aozT -> structurallyTerminating n_aozT
      }
end Rec }
\end{lstlisting}

The above example almost mirrors the source.
It calls itself recursively until the \texttt{Z} base case.

When we enable the optimization by adding \texttt{\{-\# ANN structurallyTerminating OptimizeSingleton \#-\}}, instead, the body of the function is simply replaced by a function that ignores its argument and returns \texttt{()}:

\begin{lstlisting}[language=]
-- RHS size: {terms: 2, types: 1, coercions: 0, joins: 0/0}
structurallyTerminating [InlPrag=NOINLINE] :: N -> ()
[GblId, Arity=1, Caf=NoCafRefs, Unf=OtherCon []]
structurallyTerminating = \ _ [Occ=Dead] -> GHC.Tuple.()
\end{lstlisting}

The plugin recognized that \texttt{()} has only one inhabitant, that the function is total, and that the argument is inhabited, and was able to perform the optimization.

In this example, the singleton has no type parameters, so there's no need to add coercions.
When optimizing more complex singletons, for example \texttt{:$\sim$:}, we would see a coercion to the appropriate instantiation of type parameters.

\section{Benchmarks}
\label{sec:benchmarks}

We now proceed to analyze the effects of the application of the optimization on our motivating example.

We ran a benchmark in which \texttt{Vec}s of progressively larger sizes were reversed using the \texttt{reverse} function defined in listing~\ref{lst:reversal-final}.
We then ran the same benchmark after adding the \texttt{OptimizeSingleton} annotation to the \texttt{proofIdentityZ} and \texttt{proofS} proofs, bringing their computational complexities down to a constant.
Gone the overhead of the proofs, we expect the complexity of the optimized \texttt{reverse} to be $O(n)$.

This is confirmed by the results of the benchmark, shown in figure~\ref{fig:bench-reverse}.

\begin{figure}
  \begin{minipage}{0.5\textwidth}
    \begin{tikzpicture}
    \begin{axis}
      [ title = Unoptimized list reverse
      , xlabel = List length
      , ylabel = Time (s)
      , error bars/y dir = both
      , error bars/y explicit
      , error bars/error bar style = { line width=0.5pt }
      , error bars/error mark options =
          { rotate=90
          , red
          , mark size=4pt
          , line width=0.5pt
          }
      ]
      \addplot table
        [ x = N
        , y = Mean
        , y error = Stddev
        , col sep = comma
        , only marks
        ]
        {benchmark-unoptimized.csv};
      \addplot[mark=none, domain=10000:40000]{1.449e-8*x^2 - 1.884e-4*x + 1.411};
    \end{axis}
    \end{tikzpicture}
    \label{fig:bench-reverse:unoptimized}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
    \begin{tikzpicture}
    \begin{axis}
      [ title = Optimized list reverse
      , xlabel = List length
      , ylabel = Time(s)
      , error bars/y dir = both
      , error bars/y explicit
      , error bars/error bar style = { line width=0.5pt }
      , error bars/error mark options =
          { rotate=90
          , red
          , mark size=4pt
          , line width=0.5pt
          }
      ]
      \addplot table
        [ x = N
        , y = Mean
        , y error = Stddev
        , col sep = comma
        , only marks
        ]
        {benchmark-optimized.csv};
      \addplot[mark=none, domain=10000:40000]{2.209e-7*x - 0.002};
    \end{axis}
    \end{tikzpicture}
    \label{fig:bench-reverse:optimized}
  \end{minipage}
  \caption{Measured times for a length-indexed vector reverse on vectors of varying sizes, with appropriate quadratic and linear fits. On the left, the unoptimized $O(n^2)$ version. On the right, the optimized $O(n)$ one. $R^2$ values are respectively $0.999$ and $0.992$. Note the difference in scale.}
  \label{fig:bench-reverse}
\end{figure}

In the measured range the optimized version is three orders of magnitude faster that the unoptimized one, but most importantly we can confirm the predicted complexities by fitting the data with the respective polynomials (quadratic and linear) and obtaining a low error.

This benchmark can be repeated by running the \texttt{reverse} benchmark included in the plugin's package with the command \texttt{cabal bench}.

\section{Known limitations}
\label{sec:limitations}

While we believe the implementation offers enough functionality to be useful in a practical setting, there are a few limitations to be considered.

\subsection{The optimization is restricted to individual modules}
\label{subsec:single-module}

Due to limitations in the GHC plugin system the optimizer cannot perform intra-module termination analysis, so the optimization will trigger only if all the names referenced by the annotated binding are either whitelisted or defined in the same module.

\subsection{Increased laziness}
\label{subsec:limitations-laziness}

A side effect of replacing a function body with a constant is that the arguments do not get evaluated at all.
As explained in Section~\ref{cha:solution}, this does not cause problems with the soundness of the proof, but it does cause an increase in the function's laziness.
For example, passing \texttt{undefined} as an argument will not make the optimized program crash when the unoptimized one would have.
We do not consider this as a significant problem, since the programmer is only interested in the proof, which is guaranteed to hold anyway.

\subsection{Limitations derived from LiquidHaskell}
\label{subsec:limitations-lh}

In some rare cases, LiquidHaskell's totality checker can report a function as total when it actually is not.

At the time of writing, there is only one known instance of this problem: datatype-encoded recursion.

For example, the following code is detected as total and the \texttt{callsRecWithData} obviously incorrect equality is optimized:

\begin{lstlisting}
newtype Rec a = MkRec (Rec a -> a)

recWithData :: Rec a -> a
recWithData r@(MkRec f) = f r

{-# ANN callsRecWithData OptimizeSingleton #-}
callsRecWithData :: (Int :~: Bool)
callsRecWithData = recWithData (MkRec recWithData)
\end{lstlisting}

This is a well known trick to achieve nontermination without using recursion in terms but using negative recursion in types.

This error is a known bug in LiquidHaskell\footnote{\url{https://github.com/ucsd-progsys/liquidhaskell/issues/159}}.

\chapter{Conclusions}
\label{cha:conclusions}

We showed how singleton proofs can be reduced to a constant when they are proved total and explained why this optimization isn't already done by GHC.
We developed the \texttt{singleton-optimizer} package, which can detect those cases through the use of state-of-the-art tools, and can optimize them while maintaining type soundness.
Finally, we showed that the plugin has a practical and measurable effect.

We believe that the advanced type system features combined with similarly advanced analysis techniques can not only greatly enhance the expressivity of a language, but often can do so at no additional computational cost or can even provide the means for further optimizations.

Furthermore, as shown in this work, they can be applied to a restricted part of the program to great effect, without overly restricting the rest of it (in this optimization the totality check was local only).

